{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Gender prediction",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forrestpark/NLPwithDeepLearning/blob/main/Gender_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEwSMJSGHjcb"
      },
      "source": [
        "# Preperation\n",
        "\n",
        "Importing necessary libraries, such as `pandas`, `gensim`, `torch` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWHO7X6hvxqj"
      },
      "source": [
        "# for reading a dataset\n",
        "import pandas as pd\n",
        "\n",
        "# for pre-training an embedding\n",
        "import gensim\n",
        "from gensim.models import FastText\n",
        "\n",
        "# for building and training neural networks\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as I\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmq77pTMID_Z",
        "outputId": "96c4e0ff-e3f1-4fb1-8886-95f0d2d854f6"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"using gpu for computation\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using gpu for computation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z14F-XXZ3wfZ"
      },
      "source": [
        "### Hyperparmeters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7GzS1y23yu-"
      },
      "source": [
        "embeddings_dim = 512   # fasttext embedding size\n",
        "hidden_size = 256\n",
        "depth = 4\n",
        "\n",
        "learning_rate = 0.00025\n",
        "\n",
        "embeddings_epoch = 10 # fasttext training epochs \n",
        "num_epochs = 55\n",
        "dropout = 0.8\n",
        "\n",
        "n_classes = 2         # F/M\n",
        "batch_size = 32\n",
        "print_every = 5"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kStY9GZIej0"
      },
      "source": [
        "# Corpus\n",
        "\n",
        "Downloading dataset named [Gender by Name](https://archive.ics.uci.edu/ml/datasets/Gender+by+Name)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGdXFmAPtqFR",
        "outputId": "613c25d5-1c09-4938-dd29-96087cfb6130"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00591/name_gender_dataset.csv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-08 08:26:23--  https://archive.ics.uci.edu/ml/machine-learning-databases/00591/name_gender_dataset.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3774591 (3.6M) [application/x-httpd-php]\n",
            "Saving to: ‘name_gender_dataset.csv’\n",
            "\n",
            "name_gender_dataset 100%[===================>]   3.60M  15.9MB/s    in 0.2s    \n",
            "\n",
            "2021-08-08 08:26:23 (15.9 MB/s) - ‘name_gender_dataset.csv’ saved [3774591/3774591]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqfbD6M7uDpN"
      },
      "source": [
        "Using `pandas` `read.csv()` function to read csv files\n",
        "By calling `DataFrame.head()` method, we may observe that the imported dataset consists of four fields, namely Name, Gender, Count, and Probability. The dataset set has both Name and Gender, thus it is possible that we pursue a supervised neural network learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv5oGXibHknQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "0fdd8813-63a0-44e4-a5ff-9b49fce2d4cc"
      },
      "source": [
        "data = pd.read_csv('name_gender_dataset.csv')\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Count</th>\n",
              "      <th>Probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>James</td>\n",
              "      <td>M</td>\n",
              "      <td>5304407</td>\n",
              "      <td>0.014517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>John</td>\n",
              "      <td>M</td>\n",
              "      <td>5260831</td>\n",
              "      <td>0.014398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Robert</td>\n",
              "      <td>M</td>\n",
              "      <td>4970386</td>\n",
              "      <td>0.013603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Michael</td>\n",
              "      <td>M</td>\n",
              "      <td>4579950</td>\n",
              "      <td>0.012534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>William</td>\n",
              "      <td>M</td>\n",
              "      <td>4226608</td>\n",
              "      <td>0.011567</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Name Gender    Count  Probability\n",
              "0    James      M  5304407     0.014517\n",
              "1     John      M  5260831     0.014398\n",
              "2   Robert      M  4970386     0.013603\n",
              "3  Michael      M  4579950     0.012534\n",
              "4  William      M  4226608     0.011567"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCylISlbIn9W"
      },
      "source": [
        "To express each name as a vector, a word embedding model is necessary. Corpus for embedding model training is in the format of a list of sentences, and each name input would be a sentence in this case.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "각 이름을 벡터로 표현하기 위해서는 단어 임베딩 모형이 필요하다.  임베딩 모형을 훈련하기 위한 코퍼스는 문장(단어들의 리스트)들의 리스트로 표현한다. 여기에서는 이름 하나를 문장 하나로 취급한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai8B4QmBIkKD"
      },
      "source": [
        "names = list(data['Name']) # list of names\n",
        "names = [[name] for name in names] # list of lists"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAVW_aejHcfv"
      },
      "source": [
        "# Pre-training a word embedding\n",
        "\n",
        "We hereby train a `FastText` model, which embeds each word in the given corpus as a 100-dimension vector.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "주어진 코퍼스로부터 단어들을 각각 100차원 벡터로 임베딩하는 `FastText` 모형을 훈련시킨다. 훈련과 관련된 하이퍼패러미터들의 값은 이후 검증 집합에서의 성능을 살펴보면서 변경할 수 있다. `help(FastText)`로 알아보라."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACqFAzpHvunH"
      },
      "source": [
        "embeddings = FastText(sentences=names, sg=1, min_count=1, workers=2,\n",
        "                      size=embeddings_dim, min_n=1, max_n=5,\n",
        "                      iter=embeddings_epoch)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElZpQgRRHq3O"
      },
      "source": [
        "# Building a neural network\n",
        "\n",
        "Storing as variables the diemnsion of the word embedding vector (`d`) and the number of classes (`n_classes`) to determine the dimensions of the input layer and output layer.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "입력층과 출력층의 차원을 결정하기 위해 단어 임베딩 벡터의 차원(`d`)과 분류할 범주의 개수(`n_classes`)를 변수로 저장한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wxVsdbPKKwo"
      },
      "source": [
        "Let us try 128 units for the hidden layer for experimentation's sake. We can always change the number of hidden layers as we examine the model's accuracy in the validation set.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "은닉층 유닛 개수는 128로 해 보자. 이 값은 이후 검증 집합에서의 성능을 살펴보면서 변경할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZGzsY3QKwmt"
      },
      "source": [
        "Let us create a neural network with one input layer, one hidden layer and one output layer.\n",
        "\n",
        "+  `fc1`:  Input layer --> hidden layer; ReLU for the activation function\n",
        "+  `fc2`: Hidden layer --> output layer \n",
        "\n",
        "---\n",
        "입력층, 은닉층, 출력층 각 한 개로 이루어진 신경망을 구성하자.\n",
        "\n",
        "+  `fc1`:  입력층-->은닉층. 활성화함수로 ReLU를 사용한다.\n",
        "+  `fc2`: 은닉층-->출력층.  \n",
        "\n",
        "은닉층의 개수는 이후 검증 집합에서의 성능을 살펴보면서 변경할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWbJHfouxtIa"
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    def __init__(self, depth, input_dim, hidden_dim, output_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.inp = nn.Linear(input_dim, hidden_dim)\n",
        "        self.mid = nn.ModuleList(\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "            for i in range(depth - 2))\n",
        "        self.out = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        self.norm = nn.ModuleList(\n",
        "            nn.BatchNorm1d(hidden_dim)\n",
        "            for i in range(depth - 1))\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        I.kaiming_uniform_(self.inp.weight, nonlinearity=\"relu\")\n",
        "        for layer in self.mid:\n",
        "            I.kaiming_uniform_(layer.weight, nonlinearity=\"relu\")\n",
        "        I.xavier_uniform_(self.out.weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.dropout(F.relu(self.norm[0](self.inp(x))))\n",
        "        for mid, norm in zip(self.mid, self.norm[1:]):\n",
        "            x = self.dropout(F.relu(norm(mid(x))))\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5r4H0PdL8uj"
      },
      "source": [
        "\n",
        "Store the neural network as a variable `net`\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "신경망을 `net`이라는 변수로 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ09WIKl-wRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f529b6-0912-4597-cff6-ae8ea8f59665"
      },
      "source": [
        "net = FFNN(depth=depth,\n",
        "           input_dim=embeddings_dim,\n",
        "           hidden_dim=hidden_size,\n",
        "           output_dim=n_classes,\n",
        "           dropout=dropout)\n",
        "net.to(device)\n",
        "print(net)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (inp): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (mid): ModuleList(\n",
            "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
            "  (norm): ModuleList(\n",
            "    (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.8, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYrQDiQOMsyX"
      },
      "source": [
        "입력 벡터를 실제로 신경망에 넣어서 출력 벡터를 뽑아 보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjlW18suxZgT"
      },
      "source": [
        "###  Creating an input word vector for the name 'Cyrill'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RVko_tt0KT4"
      },
      "source": [
        "input = torch.tensor(embeddings.wv['Cyrill'], device=device) # edit this line"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YF_RsHFNEw-"
      },
      "source": [
        "The input vector consists of one word vector in the 100th dimension, hence 100-dimension in total.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "입력 벡터는 100차원짜리 단어 벡터 1개로 이루어져 있으므로 100차원이 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSpuYgODNE7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8907d1b1-5507-4f0c-d012-54d83edc3496"
      },
      "source": [
        "print(input.size())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceRGm79xNUQB"
      },
      "source": [
        "We have to classes to which all inputs get classfied, hence a two-dimension vector for the output layer.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "분류할 범주 목록은 {F, M}  두 가지로 이루어져 있으므로 출력 벡터는 2차원이 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSpqIzkvALq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26645403-5e2e-4b56-94ce-57d085857b91"
      },
      "source": [
        "net.eval()\n",
        "output = net(input.unsqueeze(0))\n",
        "print(output.squeeze().size())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db4l5lPkNlBv"
      },
      "source": [
        "# Training, validation & test datasets\n",
        "\n",
        "Let us compose a dataset proper for the language model's objective. We create a new class by inheriting `torch.utils.data.Dataset` and initialize new methods `__len__()` and `__getitem__()`.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "언어 모형의 목적에 맞는 데이터셋을 구성하자. `torch.utils.data.Dataset` 클래스를 상속하여 새 클래스를 만들고, `__len__()` 함수와 `__getitem__()` 함수를 새로 만들면 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m92WN0sVyBaF"
      },
      "source": [
        "Defining the method `__getitem__()`. The value of the variable `label` should be 0 for female and 1 for male."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tZJpZehJLlc"
      },
      "source": [
        "class NameDataset(Dataset):\n",
        "    def __init__(self, data, embeddings):\n",
        "        self.names = torch.tensor([embeddings.wv[name][0] for name in names],\n",
        "                                  device=device)\n",
        "        self.labels = torch.tensor(\n",
        "            [0 if label == \"F\" else 1 for label in data['Gender']],\n",
        "            dtype=int,\n",
        "            device=device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.names[idx], self.labels[idx]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i3-WobAOESg"
      },
      "source": [
        "I was able to extract 147,269 units of data by constructing corpus from the dataset imported above and following the instructions above. Each datum consists of a 100-dimension input vector and one answer label.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "위와 같은 형식으로 코퍼스에서 데이터셋을 구축한 결과 총 147269개의 데이터가 나왔다.  각 데이터는 100차원의 입력 벡터와 1개의 정답 레이블로 이루어져 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdIRC6o4LmnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fcc15a-bdea-4f3f-8a9b-60748cf07174"
      },
      "source": [
        "dataset = NameDataset(data=data, embeddings=embeddings)\n",
        "print(len(dataset))\n",
        "print(dataset[0][:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "147269\n",
            "(tensor([-1.7508e-04, -5.8746e-05, -1.5219e-04,  8.0608e-05, -1.2216e-05,\n",
            "        -1.7953e-04, -1.4539e-04, -3.3422e-04,  2.8326e-04, -3.2299e-05,\n",
            "        -8.7766e-05, -1.1470e-04, -7.3571e-05, -5.8004e-05, -2.1175e-04,\n",
            "        -7.6971e-06, -3.3010e-05, -6.7060e-06, -9.6139e-05, -8.8557e-05,\n",
            "         3.2708e-05, -1.9642e-04, -2.3934e-04,  1.5144e-04, -9.0619e-06,\n",
            "        -2.4235e-04,  2.7663e-04,  3.8188e-04, -2.1025e-04, -7.1081e-06,\n",
            "        -6.3145e-05, -1.8900e-04,  4.0729e-04,  1.4545e-04, -1.9654e-04,\n",
            "        -6.0527e-04,  2.5291e-04,  1.2834e-04, -2.1131e-04,  3.1024e-04,\n",
            "        -9.7725e-05,  1.2811e-04,  1.2853e-04, -1.1167e-04, -3.0214e-04,\n",
            "         8.2110e-05,  2.3547e-04, -6.6721e-05,  1.8263e-05,  9.0922e-05,\n",
            "        -1.9627e-04,  4.0097e-05,  8.6036e-05,  1.8092e-04,  7.6385e-05,\n",
            "         8.7623e-05, -1.3785e-04, -7.5645e-05,  3.7484e-04,  2.2766e-04,\n",
            "         6.3754e-05, -1.6311e-04,  3.3755e-04,  2.6203e-04, -1.4301e-04,\n",
            "         1.3966e-04,  1.9825e-04, -2.8852e-05,  1.9015e-04,  4.7525e-05,\n",
            "         2.9101e-05,  7.3856e-06, -1.4750e-04, -1.9994e-04,  3.5063e-04,\n",
            "        -8.8483e-05,  4.0992e-05, -1.3086e-04, -1.5765e-04,  5.9822e-04,\n",
            "         2.7883e-04, -4.1668e-05,  1.2376e-04, -2.3106e-04, -2.1265e-04,\n",
            "         1.9024e-04, -3.2181e-04,  2.6014e-05,  3.7540e-05, -8.5272e-05,\n",
            "         2.5565e-04, -5.1068e-05,  7.1193e-05, -9.9219e-06,  1.1292e-04,\n",
            "        -2.0719e-04,  2.2112e-04, -3.7866e-04, -4.0750e-04, -5.7674e-05,\n",
            "        -2.9413e-04, -2.7517e-04, -3.6763e-05,  2.7415e-04,  3.3167e-05,\n",
            "        -1.4678e-04, -2.7471e-04,  2.8745e-04,  3.2091e-04,  1.9097e-04,\n",
            "        -1.1533e-05, -1.8868e-04,  7.3353e-05, -2.4633e-04, -9.5028e-05,\n",
            "         1.4301e-05, -2.1486e-04, -1.2352e-04,  2.5507e-04,  1.2763e-05,\n",
            "        -1.1058e-04,  7.0239e-05, -3.2400e-04,  2.3486e-05, -1.0514e-04,\n",
            "         6.2192e-06, -2.4007e-05,  4.0323e-04,  8.7757e-05, -1.7633e-04,\n",
            "         5.9948e-05,  5.8273e-04, -5.4546e-04,  7.9442e-05, -1.0499e-04,\n",
            "         3.9366e-04, -2.0179e-04,  7.2893e-05,  5.0863e-05,  3.0564e-04,\n",
            "        -1.1681e-04,  2.1571e-04, -1.5290e-04, -1.2109e-04, -1.2405e-04,\n",
            "        -2.4438e-05, -2.4334e-05, -2.1731e-04, -2.9903e-04, -1.7345e-06,\n",
            "         1.1213e-04, -4.1270e-04, -1.5601e-04,  8.7492e-05, -2.9198e-04,\n",
            "        -2.6451e-04,  2.9220e-04, -1.7170e-04, -8.6653e-05,  9.3862e-05,\n",
            "         1.2802e-04, -1.8719e-04, -4.0427e-05,  6.5274e-05,  9.8967e-05,\n",
            "        -4.2644e-04,  1.2584e-04,  1.0394e-05, -2.9585e-05, -3.8949e-04,\n",
            "        -1.2489e-04, -3.2885e-04, -1.5316e-05,  4.7151e-05, -9.9499e-05,\n",
            "         1.0941e-04,  2.7319e-04,  6.7344e-05, -1.0537e-04, -4.5939e-05,\n",
            "        -2.4570e-04, -3.2791e-04,  1.8507e-04,  4.2718e-05,  4.1226e-04,\n",
            "        -7.1921e-05,  1.0975e-04,  2.2316e-04,  1.9311e-04,  2.3808e-04,\n",
            "         5.3449e-04, -2.8571e-04, -2.1814e-04, -2.7306e-04,  1.9686e-04,\n",
            "        -1.7100e-04, -1.9033e-04, -1.5142e-04,  1.5201e-04, -1.9484e-04,\n",
            "         4.6292e-04, -6.1887e-05,  1.0323e-05, -2.4730e-04, -1.4662e-04,\n",
            "        -2.6777e-04,  2.7145e-04,  3.8593e-06,  2.8421e-06,  1.1535e-04,\n",
            "        -4.0892e-05,  9.0428e-05, -2.6988e-04,  4.0869e-04, -3.8933e-04,\n",
            "         7.0734e-05, -1.7135e-04, -7.7388e-05,  4.1658e-04, -5.6427e-04,\n",
            "        -1.6013e-04, -2.5478e-04, -2.7608e-04, -9.0209e-05,  1.9701e-04,\n",
            "        -8.8058e-05,  2.9362e-05,  8.1075e-05, -2.9167e-05, -1.2092e-05,\n",
            "        -1.1609e-04,  2.8121e-04,  1.7548e-04, -6.9904e-05, -1.7297e-04,\n",
            "        -2.3823e-04, -2.5008e-04,  1.8577e-05, -4.2987e-04, -1.4462e-04,\n",
            "         9.8615e-06,  1.5306e-04,  3.4416e-04, -1.8987e-04, -3.0620e-04,\n",
            "         9.6721e-05, -2.3994e-04,  2.9539e-05,  1.2995e-04, -5.6310e-05,\n",
            "         1.7968e-04, -5.2340e-05,  3.5439e-05,  5.4073e-05, -1.1091e-04,\n",
            "        -1.1176e-04,  1.3370e-04, -2.3229e-06, -4.0984e-04,  1.0668e-04,\n",
            "         5.6931e-05,  4.9286e-04,  1.8791e-04, -3.1265e-05,  1.0356e-05,\n",
            "         2.3155e-05,  2.4917e-04, -1.7763e-04, -9.3218e-05,  2.1061e-04,\n",
            "         8.5411e-05, -1.6701e-04, -2.4345e-04,  2.7515e-04, -1.1792e-04,\n",
            "        -2.8726e-04,  2.9469e-04, -9.2570e-06, -2.5889e-05,  8.8708e-05,\n",
            "         2.4771e-04, -1.5396e-04,  4.2219e-04, -2.4202e-04, -4.2679e-05,\n",
            "         7.4079e-05, -2.9436e-04,  2.1233e-05, -2.0383e-04, -9.0199e-05,\n",
            "        -2.1931e-04,  4.4211e-04,  5.3569e-05, -2.6566e-04, -5.1637e-04,\n",
            "         1.2790e-04, -1.5164e-04, -1.0024e-04,  8.3769e-05, -2.0197e-04,\n",
            "        -7.0268e-05,  1.3587e-04, -4.1875e-04,  1.1755e-05,  1.3394e-04,\n",
            "        -1.4391e-04, -2.2633e-04,  4.3279e-05, -4.7792e-04, -5.8250e-05,\n",
            "        -1.7907e-04, -1.1173e-04,  2.5230e-04,  2.5640e-05,  1.1968e-04,\n",
            "         3.3215e-04, -1.5007e-04,  1.2910e-04, -1.6049e-04,  1.1269e-04,\n",
            "         2.3851e-05,  7.8884e-06,  6.1462e-04, -5.3539e-04,  3.2520e-04,\n",
            "        -1.0135e-04, -2.9872e-04, -8.0725e-05,  2.1586e-04,  2.4009e-04,\n",
            "        -1.5912e-05, -7.4157e-05, -1.7226e-04, -2.5819e-04,  2.3890e-04,\n",
            "        -4.6285e-04, -1.1370e-04, -1.2744e-04, -4.3604e-05, -1.1933e-04,\n",
            "        -8.5545e-05,  2.3549e-04, -8.2983e-05,  1.6909e-05, -2.5578e-04,\n",
            "         2.3628e-05, -2.3830e-05,  2.0932e-04,  1.6498e-04, -1.7983e-04,\n",
            "        -3.4849e-04,  1.7221e-04, -4.4111e-05,  3.8212e-05, -2.2309e-04,\n",
            "         1.1719e-04, -5.5797e-05, -1.0746e-04,  1.6139e-04,  1.6739e-04,\n",
            "        -3.6023e-04, -8.8467e-05,  1.3328e-04,  1.4898e-04,  1.4349e-04,\n",
            "         5.2131e-05, -1.7558e-04, -1.7377e-04,  2.3820e-04, -1.4553e-04,\n",
            "         2.6641e-05, -3.4890e-04, -8.9133e-05,  7.8659e-05, -1.4217e-04,\n",
            "         9.7333e-05, -1.2954e-04,  9.5689e-05, -5.2247e-05, -2.2215e-05,\n",
            "         1.9340e-04, -2.4966e-05,  1.5450e-05, -2.2651e-04,  2.2436e-05,\n",
            "        -2.6757e-04,  3.0366e-04,  2.3262e-04,  2.0905e-04, -2.5565e-04,\n",
            "        -1.4639e-04,  6.3363e-05,  1.2218e-04, -1.3473e-04,  3.6705e-04,\n",
            "        -2.1737e-05, -9.4844e-05, -9.9381e-05, -2.7479e-04,  1.5536e-06,\n",
            "         6.4457e-05,  5.5479e-05, -4.1805e-04,  5.2683e-04,  3.5820e-04,\n",
            "        -2.5547e-05,  1.1129e-04, -3.9774e-04,  2.7479e-04, -8.9836e-05,\n",
            "        -1.1410e-04, -1.6234e-04,  6.9228e-05, -6.9987e-05, -3.2890e-04,\n",
            "         8.1160e-05,  7.2803e-04, -5.6709e-05,  5.4872e-06, -2.9382e-04,\n",
            "        -2.2360e-04,  4.9571e-05, -2.2619e-04, -3.0972e-04,  2.6547e-05,\n",
            "        -1.2932e-06, -8.3084e-05,  1.2015e-04,  1.4437e-04, -2.2666e-04,\n",
            "         2.5815e-04,  2.3217e-04,  1.0253e-04, -5.7236e-06, -1.9037e-04,\n",
            "        -1.5717e-04, -1.3382e-04,  2.0985e-06, -2.7165e-04,  7.2255e-05,\n",
            "        -3.3432e-04,  1.3171e-05, -2.1285e-04, -1.5753e-04,  1.9832e-04,\n",
            "         1.8384e-04, -3.0363e-05, -2.6087e-04,  5.9447e-05, -1.0907e-04,\n",
            "         1.4464e-04, -2.5896e-04,  1.9125e-04, -3.3926e-04,  2.2688e-04,\n",
            "        -4.4961e-05,  1.5230e-06,  3.9752e-04,  2.1133e-04, -2.1743e-04,\n",
            "         9.4605e-05, -6.7089e-05, -4.1690e-05,  1.3214e-04,  2.0055e-04,\n",
            "         2.0493e-04,  1.9374e-04, -1.2049e-04,  7.5202e-05, -3.1667e-04,\n",
            "         3.5168e-05, -3.9998e-04,  1.5012e-04,  8.4978e-05,  4.2674e-04,\n",
            "        -3.9168e-04,  3.2799e-04,  4.3813e-04, -3.3611e-04,  3.4438e-04,\n",
            "         1.4549e-04, -4.7630e-04, -4.2757e-05, -5.6628e-04,  2.0482e-04,\n",
            "         8.6747e-05, -9.8436e-05, -1.6189e-04,  3.4696e-05, -1.5190e-04,\n",
            "        -3.1018e-04,  3.2249e-05, -1.3770e-04, -1.9229e-04, -9.9082e-05,\n",
            "         2.3820e-04, -1.1793e-05, -2.2480e-04,  5.1902e-04, -2.7504e-05,\n",
            "        -2.9752e-04, -2.1713e-04,  2.2333e-05,  4.1954e-05, -1.0923e-04,\n",
            "         2.2119e-04,  2.2236e-04,  2.9530e-04, -2.5076e-05, -7.8661e-05,\n",
            "        -2.1878e-04,  1.9654e-04], device='cuda:0'), tensor(1, device='cuda:0'))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--QqyLScOXsZ"
      },
      "source": [
        "In the code below,\n",
        "1. I set batch size to 32,\n",
        "2. split our data into three sets, using `torch.utils.data.random_split()`. Those three sets are the training set (120,000 entries),  the validation set (10,000 entries),  and the experimentation set(the rest),\n",
        "3. and I also create a `DataLoader()` object that reads data from each set in the size of the batch size set above.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "아래 코드에서는\n",
        "\n",
        "1. 배치 사이즈를 32로 설정한다.\n",
        "2. `torch.utils.data.random_split()` 함수로 훈련 집합(120000개),  검증 집합(10000개),  실험 집합(나머지)을 분할한다.\n",
        "3. 각 집합을 한 번에 배치 사이즈만큼 읽어 오는 `DataLoader()` 객체를 만든다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "108AXUOGMcV6"
      },
      "source": [
        "train_dataset, valid_dataset, test_dataset = random_split(\n",
        "    dataset=dataset,\n",
        "    lengths=[120000, 10000, len(dataset)-130000],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZQxOOTYOu_N"
      },
      "source": [
        "# Training & evaluating the neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwfhqXNIPSIU"
      },
      "source": [
        "##  Loss function\n",
        "\n",
        "신경망 훈련을 위해 교차 엔트로피 손실함수를 가져온다. 실제로는 먼저 소프트맥스 활성화함수를 적용한 후에 L_CE를 계산하는 구조로 되어 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JXFbZPIO3vw"
      },
      "source": [
        "# criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgg1RYMtPPiY"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "최적화기를 Adam으로 사용하고 초기 학습률을 0.01로 설정한다. 최적화기와 학습률은 이후 검증 집합에서의 성능을 살펴보면서 변경할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca2rE1jaGCQW"
      },
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5,\n",
        "                                                 verbose=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB5VvZ2tPm2F"
      },
      "source": [
        "## Training & Evaluating\n",
        "\n",
        "Based on the analysis on training data done using the cross entropy loss function and an optimizer, we update the neural network's hyperparameters and check accuracy of the neural model based on its accuracy on validation set. The `epoch` hyperparameter may be altered.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "손실함수와 최적화기를 사용하여 훈련 집합의 데이터로 신경망의 가중치 매개변수를 업데이트하고, 검증 집합에서의 정확도(accuracy)로 성능을 확인한다. epoch 횟수는 이후 검증 집합에서의 성능을 살펴보면서 변경할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SomO1_4w0Vjf"
      },
      "source": [
        "###  Writing a function that calculates accuracy on validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyFtzjdDONVH"
      },
      "source": [
        "def train(net, optimizer, loader, epoch):\n",
        "    net.train()\n",
        "    correct = sum_loss = 0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        labels = labels.to(outputs.device)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = F.cross_entropy(outputs, labels, reduction=\"sum\")\n",
        "        sum_loss += loss\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predict = outputs.argmax(-1) == labels\n",
        "        correct += sum(predict)\n",
        "\n",
        "    avg_loss = sum_loss / len(loader.dataset)\n",
        "    accuracy = correct / len(loader.dataset)\n",
        "\n",
        "    if epoch == 1 or not epoch % print_every:\n",
        "        print(\n",
        "            f\"Epoch: {epoch}. Train Loss: {avg_loss.item()}. \"\n",
        "            f\"Train Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "\n",
        "def validate(net, loader, scheduler, eval_type, epoch):\n",
        "    correct = loss = 0\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "\n",
        "        for (inputs, labels) in loader:\n",
        "            outputs = net(inputs)\n",
        "            labels = labels.to(outputs.device)\n",
        "\n",
        "            loss += F.cross_entropy(outputs, labels, reduction=\"sum\")\n",
        "            predict = outputs.argmax(-1) == labels\n",
        "            correct += sum(predict)\n",
        "\n",
        "        loss = loss / len(loader.dataset)\n",
        "        accuracy = correct / len(loader.dataset)\n",
        "        if scheduler:\n",
        "            scheduler.step(loss)\n",
        "\n",
        "    if epoch == 1 or not epoch % print_every:\n",
        "        print(\n",
        "            f\"Epoch: {epoch}. {eval_type} Loss: {loss.item()}. \"\n",
        "            f\"{eval_type} Accuracy: {accuracy:.2%}\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g919VhEG5mwA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c264b5-7fc4-4ff0-dc33-2f215ecc765d"
      },
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(net, optimizer, train_loader, epoch)\n",
        "    validate(net, valid_loader, scheduler, \"Validation\", epoch)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1. Train Loss: 0.7710070013999939. Train Accuracy: 58.23%\n",
            "Epoch: 1. Validation Loss: 0.6138774752616882. Validation Accuracy: 65.23%\n",
            "Epoch: 5. Train Loss: 0.5158765912055969. Train Accuracy: 75.33%\n",
            "Epoch: 5. Validation Loss: 0.4951203465461731. Validation Accuracy: 76.07%\n",
            "Epoch: 10. Train Loss: 0.49089348316192627. Train Accuracy: 76.98%\n",
            "Epoch: 10. Validation Loss: 0.48478788137435913. Validation Accuracy: 76.76%\n",
            "Epoch: 15. Train Loss: 0.47643008828163147. Train Accuracy: 77.98%\n",
            "Epoch: 15. Validation Loss: 0.4685654044151306. Validation Accuracy: 78.19%\n",
            "Epoch: 20. Train Loss: 0.4668131172657013. Train Accuracy: 78.66%\n",
            "Epoch: 20. Validation Loss: 0.4639536440372467. Validation Accuracy: 78.05%\n",
            "Epoch: 25. Train Loss: 0.46067723631858826. Train Accuracy: 78.89%\n",
            "Epoch: 25. Validation Loss: 0.46031635999679565. Validation Accuracy: 78.47%\n",
            "Epoch: 30. Train Loss: 0.4529385566711426. Train Accuracy: 79.34%\n",
            "Epoch: 30. Validation Loss: 0.4830315411090851. Validation Accuracy: 76.53%\n",
            "Epoch: 35. Train Loss: 0.44782525300979614. Train Accuracy: 79.62%\n",
            "Epoch    35: reducing learning rate of group 0 to 2.5000e-05.\n",
            "Epoch: 35. Validation Loss: 0.47606903314590454. Validation Accuracy: 77.46%\n",
            "Epoch: 40. Train Loss: 0.4418247640132904. Train Accuracy: 79.95%\n",
            "Epoch: 40. Validation Loss: 0.4441637098789215. Validation Accuracy: 79.83%\n",
            "Epoch    42: reducing learning rate of group 0 to 2.5000e-06.\n",
            "Epoch: 45. Train Loss: 0.4409710466861725. Train Accuracy: 80.04%\n",
            "Epoch: 45. Validation Loss: 0.44439882040023804. Validation Accuracy: 79.89%\n",
            "Epoch    48: reducing learning rate of group 0 to 2.5000e-07.\n",
            "Epoch: 50. Train Loss: 0.43935808539390564. Train Accuracy: 80.17%\n",
            "Epoch: 50. Validation Loss: 0.4438973069190979. Validation Accuracy: 79.64%\n",
            "Epoch: 55. Train Loss: 0.44003036618232727. Train Accuracy: 80.11%\n",
            "Epoch: 55. Validation Loss: 0.4462648332118988. Validation Accuracy: 79.28%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soYNlQML0pEP"
      },
      "source": [
        "Accuracy calculation on experiment set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CljkMWRVj_Kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af79b1c-2a0b-496f-d21f-ecb373f31a15"
      },
      "source": [
        "validate(net, test_loader, None, \"Test\", epoch)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 55. Test Loss: 0.44618508219718933. Test Accuracy: 79.28%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaGZoARW0zwX"
      },
      "source": [
        "### Fine-tuning and optimizing the model for maximum accuracy by adjusting hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElGNK_7AQpip"
      },
      "source": [
        "# Predicting the gender\n",
        "\n",
        "Let us check whether the neural network works for name inputs that are not in the experiment set. The name \"Shrek\" is one of such name inputs.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "실험 집합에도 없는 새로운 이름에 대해서도 신경망이 잘 작동하는지 확인해 보자.\n",
        "\n",
        "예를 들어 \"Shrek\"이라는 이름은 데이터에 없다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxqMikAbfa2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311bd8ca-9b1e-44b9-ebbc-6b1c0b2e5f6f"
      },
      "source": [
        "print(['Shrek'] in names)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1YFq4P2FRgY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9ee13dfc-fb9d-4b74-929a-8b95804189a6"
      },
      "source": [
        "net.eval()\n",
        "input = torch.tensor(embeddings.wv['Shrek'], device=device)\n",
        "output = net(input.unsqueeze(0))\n",
        "\"M\" if output.squeeze().argmax().item() else \"F\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'M'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfLDpm8t1_j2"
      },
      "source": [
        "### Let us see as what gender the model predicts the name \"Shica\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9xMciEOfRlI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f5a0a073-90e6-4e34-99e9-0473948846ad"
      },
      "source": [
        "input = torch.tensor(embeddings.wv['Shica'], device=device)\n",
        "output = net(input.unsqueeze(0))\n",
        "\"M\" if output.squeeze().argmax().item() else \"F\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'F'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}